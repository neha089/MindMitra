{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrj5HifPxAM0",
        "outputId": "5aa39b4b-19ae-4730-d9a5-e88e2f4dca1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3fyKfVTwtZ9",
        "outputId": "3b3ebe55-e10c-4dc8-bf49-d22186b4552f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6a07abe0a7a4>:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df_reduced = df.groupby(\"tag\", group_keys=False).apply(lambda x: x.sample(n=emotion_sample_sizes[x.name], random_state=42))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Dataset reduced to 100K samples while preserving emotion distribution.\n",
            "tag\n",
            "joy         27316\n",
            "sadness     24049\n",
            "fear        19833\n",
            "neutral     19728\n",
            "anger        3796\n",
            "surprise     3383\n",
            "disgust      1890\n",
            "unknown         1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = \"/content/drive/MyDrive/correct_dataset_sorted.csv\"  # Replace with your actual path\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Count occurrences of each emotion\n",
        "emotion_counts = df[\"tag\"].value_counts()\n",
        "total_samples = 100_000\n",
        "\n",
        "# Calculate sample size per emotion based on original distribution\n",
        "emotion_sample_sizes = (emotion_counts / emotion_counts.sum() * total_samples).astype(int)\n",
        "\n",
        "# Ensure at least 1 sample per category (if needed)\n",
        "emotion_sample_sizes = emotion_sample_sizes.clip(lower=1)\n",
        "\n",
        "# Sample data while maintaining emotion proportions\n",
        "df_reduced = df.groupby(\"tag\", group_keys=False).apply(lambda x: x.sample(n=emotion_sample_sizes[x.name], random_state=42))\n",
        "\n",
        "# Shuffle dataset (optional)\n",
        "df_reduced = df_reduced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the reduced dataset\n",
        "df_reduced.to_csv(\"/content/drive/MyDrive/reduced_dataset.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Dataset reduced to 100K samples while preserving emotion distribution.\")\n",
        "\n",
        "print(df_reduced[\"tag\"].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# âœ… File path for the CSV file\n",
        "file_path = \"/content/drive/MyDrive/reduced_dataset.csv\"\n",
        "\n",
        "# âœ… Output file path for the sorted CSV\n",
        "output_file = \"/content/drive/MyDrive/reduced_dataset_sorted.csv\"\n",
        "\n",
        "# âœ… Load the CSV file\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# âœ… Sort the DataFrame by the 'tag' column\n",
        "df_sorted = df.sort_values(by='tag').reset_index(drop=True)\n",
        "\n",
        "# âœ… Save the sorted DataFrame to a new CSV file\n",
        "df_sorted.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"ðŸŽ‰âœ… Dataset sorted by 'tag' column and saved to: {output_file}\")\n",
        "\n",
        "# âœ… Optional: Preview sorted data\n",
        "print(f\"ðŸ“Š Sorted dataset shape: {df_sorted.shape}\")\n",
        "print(df_sorted.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3mhY0VRywoR",
        "outputId": "bfc1966f-e8df-4ad9-ab70-03a3d2e1fea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ‰âœ… Dataset sorted by 'tag' column and saved to: /content/drive/MyDrive/reduced_dataset_sorted.csv\n",
            "ðŸ“Š Sorted dataset shape: (99996, 3)\n",
            "                                             pattern  \\\n",
            "0  Thank you for reminding me of that, Buddy. Bes...   \n",
            "1  I'm feeling frustrated and abused. It's like I...   \n",
            "2  Well, it all started with a breakdown in commu...   \n",
            "3  The guilt stems from a sense of shame I feel a...   \n",
            "4  I love my parents, and I don't want to see the...   \n",
            "\n",
            "                                            response    tag  \n",
            "0  Insecurity is a common feeling, especially whe...  anger  \n",
            "1  Hello, Fabozzi. I'm here to listen and support...  anger  \n",
            "2  The breakdown in communication must have made ...  anger  \n",
            "3  Cletus, I can sense the pain and self-judgment...  anger  \n",
            "4  That's a beautiful approach, Adalard. It shows...  anger  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the CSV file\n",
        "csv_file = \"/content/drive/MyDrive/reduced_dataset_sorted.csv\"  # Update with your actual path if needed\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# Group by 'tag' column\n",
        "grouped = df.groupby(\"tag\")\n",
        "\n",
        "train_data = []\n",
        "validation_data = []\n",
        "test_data = []\n",
        "\n",
        "for tag, group in grouped:\n",
        "    patterns = group[\"pattern\"].tolist()\n",
        "    responses = group[\"response\"].tolist()\n",
        "\n",
        "    if len(patterns) > 1:\n",
        "        # Split into training (80%) and temp (20%)\n",
        "        train_patterns, temp_patterns, train_responses, temp_responses = train_test_split(\n",
        "            patterns, responses, train_size=0.8, random_state=42\n",
        "        )\n",
        "\n",
        "        if len(temp_patterns) > 1:\n",
        "            # Split remaining into validation (50%) and test (50%)\n",
        "            validation_patterns, test_patterns, validation_responses, test_responses = train_test_split(\n",
        "                temp_patterns, temp_responses, train_size=0.5, random_state=42\n",
        "            )\n",
        "        else:\n",
        "            validation_patterns, test_patterns = temp_patterns, []\n",
        "            validation_responses, test_responses = temp_responses, []\n",
        "\n",
        "        # Add to respective lists\n",
        "        train_data.extend([{\"tag\": tag, \"pattern\": p, \"response\": r} for p, r in zip(train_patterns, train_responses)])\n",
        "        validation_data.extend([{\"tag\": tag, \"pattern\": p, \"response\": r} for p, r in zip(validation_patterns, validation_responses)])\n",
        "        test_data.extend([{\"tag\": tag, \"pattern\": p, \"response\": r} for p, r in zip(test_patterns, test_responses)])\n",
        "    else:\n",
        "        # If only one pattern exists for a tag, add it to training data\n",
        "        train_data.append({\"tag\": tag, \"pattern\": patterns[0], \"response\": responses[0]})\n",
        "\n",
        "train_df = pd.DataFrame(train_data)\n",
        "validation_df = pd.DataFrame(validation_data)\n",
        "test_df = pd.DataFrame(test_data)\n",
        "\n",
        "train_df.to_csv(\"/content/drive/MyDrive/train_data.csv\", index=False, encoding='utf-8-sig')\n",
        "validation_df.to_csv(\"/content/drive/MyDrive/validation_data.csv\", index=False, encoding='utf-8-sig')\n",
        "test_df.to_csv(\"/content/drive/MyDrive/test_data.csv\", index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(f\"Train Data saved: {len(train_df)} samples\")\n",
        "print(f\"Validation Data saved: {len(validation_df)} samples\")\n",
        "print(f\"Test Data saved: {len(test_df)} samples\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ad3kMPrKzClL",
        "outputId": "19fcce7b-1263-40c8-8c27-47fccc250f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data saved: 79994 samples\n",
            "Validation Data saved: 10000 samples\n",
            "Test Data saved: 10002 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_reduced.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Wlf_DO4xd0S",
        "outputId": "98a86e01-c8ae-4db3-ee54-888e3052e8e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             pattern  \\\n",
            "0  I plan to continue practicing self-care, Buddy...   \n",
            "1  I've started writing consistently and sharing ...   \n",
            "2  Definitely, Buddy. One surprising thing has be...   \n",
            "3  That sounds like a challenge, but it's somethi...   \n",
            "4  I have been trying to budget and cut down on u...   \n",
            "\n",
            "                                            response   tag  \n",
            "0  It's wonderful to hear that you have found cop...   joy  \n",
            "1  It's great to hear that you have taken those i...  fear  \n",
            "2  That's a profound insight, Ot. Grief indeed ha...   joy  \n",
            "3  I admire your willingness to embrace this chal...  fear  \n",
            "4  I hear you. It can be quite disheartening when...  fear  \n"
          ]
        }
      ]
    }
  ]
}